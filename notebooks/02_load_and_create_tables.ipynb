{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 - Load and Create Tables (Delta & Iceberg)\n",
        "\n",
        "Este notebook mostra um fluxo passo-a-passo para:\n",
        "1. Criar uma SparkSession configurada para **Delta Lake** e **Apache Iceberg**;\n",
        "2. Carregar um CSV de exemplo (NYC TLC Green Taxi);\n",
        "3. Criar tabelas Delta e Iceberg;\n",
        "4. Mostrar exemplos de `INSERT`, `UPDATE`, `DELETE` e `MERGE` para ambas tabelas.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Preparacao e variáveis úteis\n",
        "\n",
        "Defina (no terminal antes de abrir o Jupyter) as variáveis de ambiente para garantir que os jars do Delta e Iceberg sejam carregados pelo kernel do PySpark:\n",
        "\n",
        "```bash\n",
        "export SPARK_PACKAGES=\"io.delta:delta-core_2.12:2.2.0,org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.3.0\"\n",
        "PYSPARK_SUBMIT_ARGS=\"--packages $SPARK_PACKAGES pyspark-shell\" jupyter lab\n",
        "```\n",
        "\n",
        "No Windows PowerShell use `setx`/`$env:SPARK_PACKAGES=...` conforme necessário."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports e criação da SparkSession usando src.spark_config.build_spark\n",
        "import os, sys\n",
        "proj_root = os.path.abspath(os.path.join('..'))\n",
        "sys.path.append(os.path.abspath('../src'))\n",
        "\n",
        "try:\n",
        "    from spark_config import build_spark\n",
        "except Exception as e:\n",
        "    # caso o import falhe (por exemplo pasta não no path), tente importar pelo pacote src\n",
        "    try:\n",
        "        sys.path.append(os.path.abspath('..'))\n",
        "        from src.spark_config import build_spark\n",
        "    except Exception:\n",
        "        print(\"Não foi possível importar build_spark automaticamente. Verifique o PYTHONPATH e a estrutura do projeto.\")\n",
        "        raise\n",
        "\n",
        "spark = build_spark('pyspark-datalakes-notebook')\n",
        "print('Spark version:', spark.version)\n",
        "spark.conf.get('spark.jars.packages') if 'spark.jars.packages' in spark.conf.getAll() else None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Baixar/Localizar dados de exemplo\n",
        "Este notebook espera um CSV em `data/green_tripdata_sample.csv`. Se não existir, ele tentará baixar um subconjunto público (NYC TLC)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_path = os.path.abspath(os.path.join('..','data','green_tripdata_sample.csv'))\n",
        "os.makedirs(os.path.dirname(data_path), exist_ok=True)\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    print('Arquivo não encontrado em', data_path)\n",
        "    print('Tentando baixar uma amostra pública (se houver internet disponível)...')\n",
        "    # tentativa de download com curl (fallback para python requests não garantido)\n",
        "    url = 'https://s3.amazonaws.com/nyc-tlc/trip+data/green_tripdata_2022-01.csv'\n",
        "    cmd = f\"curl -sS -o '{data_path}' '{url}' || wget -q -O '{data_path}' '{url}'\"\n",
        "    rc = os.system(cmd)\n",
        "    if rc == 0 and os.path.exists(data_path):\n",
        "        print('Download concluído:', data_path)\n",
        "    else:\n",
        "        print('Não foi possível baixar o arquivo automaticamente. Você pode colocar um CSV em', data_path)\n",
        "else:\n",
        "    print('Dados encontrados em', data_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Ler o CSV com inferSchema (cuidado: infer pode ser lento para arquivos grandes)\n",
        "if os.path.exists(data_path):\n",
        "    trips_raw = (\n",
        "        spark.read\n",
        "        .option('header', True)\n",
        "        .option('inferSchema', True)\n",
        "        .csv(data_path)\n",
        "    )\n",
        "    print('Número de linhas (amostra):', trips_raw.count())\n",
        "    display_cols = trips_raw.columns[:12]\n",
        "    print('Colunas (amostra):', display_cols)\n",
        "    trips_raw.printSchema()\n",
        "else:\n",
        "    print('Arquivo CSV não disponível — pare aqui e adicione um CSV em data/green_tripdata_sample.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Modelagem e criação do DataFrame final `trips_df`\n",
        "Selecionamos apenas as colunas necessárias e normalizamos nomes. Também geramos um `trip_id` único."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, expr, monotonically_increasing_id, lit\n",
        "from pyspark.sql.types import StringType\n",
        "\n",
        "if 'trips_raw' in globals():\n",
        "    # adaptamos os nomes conforme a disponibilidade no CSV\n",
        "    cols = trips_raw.columns\n",
        "    # heurística: procurar colunas de pickup/dropoff datetime\n",
        "    pickup_col = next((c for c in cols if 'pickup' in c.lower()), None)\n",
        "    dropoff_col = next((c for c in cols if 'dropoff' in c.lower()), None)\n",
        "    vendor_col = next((c for c in cols if 'vendor' in c.lower()), None)\n",
        "    fare_col = next((c for c in cols if 'fare' in c.lower()), None)\n",
        "    pass_col = next((c for c in cols if 'passenger' in c.lower()), None)\n",
        "    dist_col = next((c for c in cols if 'distance' in c.lower()), None)\n",
        "\n",
        "    print('Usando colunas:', pickup_col, dropoff_col, vendor_col, fare_col, pass_col, dist_col)\n",
        "\n",
        "    trips_df = trips_raw.select(\n",
        "        expr('cast(uuid() as string) as trip_id'),\n",
        "        col(pickup_col).alias('pickup_datetime') if pickup_col is not None else lit(None).cast(StringType()).alias('pickup_datetime'),\n",
        "        col(dropoff_col).alias('dropoff_datetime') if dropoff_col is not None else lit(None).cast(StringType()).alias('dropoff_datetime'),\n",
        "        col(vendor_col).alias('vendor_id') if vendor_col is not None else lit('unknown').alias('vendor_id'),\n",
        "        col(pass_col).cast('int').alias('passenger_count') if pass_col is not None else lit(1).alias('passenger_count'),\n",
        "        col(dist_col).cast('double').alias('trip_distance') if dist_col is not None else lit(0.0).alias('trip_distance'),\n",
        "        col(fare_col).cast('double').alias('fare_amount') if fare_col is not None else lit(0.0).alias('fare_amount')\n",
        "    )\n",
        "    print('trips_df schema:')\n",
        "    trips_df.printSchema()\n",
        "    print('Exemplo de linhas:')\n",
        "    trips_df.show(5, truncate=False)\n",
        "else:\n",
        "    print('trips_raw não definido — carregue o CSV')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Criar tabela Delta (local path)\n",
        "Usamos um diretório local (`/tmp/datalake/delta_trips`) para armazenar os arquivos Delta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "delta_path = '/tmp/datalake/delta_trips'\n",
        "if 'trips_df' in globals():\n",
        "    # sobrescreve para facilitar testes iterativos\n",
        "    trips_df.write.format('delta').mode('overwrite').save(delta_path)\n",
        "    # registrar no catálogo do Spark (opcional)\n",
        "    spark.sql(f\"CREATE TABLE IF NOT EXISTS default.delta_trips USING DELTA LOCATION '{delta_path}'\")\n",
        "    print('Tabela Delta criada em', delta_path)\n",
        "    print('Contagem Delta:', spark.read.format('delta').load(delta_path).count())\n",
        "else:\n",
        "    print('trips_df não existe — não é possível criar a tabela Delta')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Criar tabela Iceberg (catalog `local_iceberg` configurado em spark_config)\n",
        "Usamos o catálogo `local_iceberg` (type=hadoop) configurado no `spark_config.py`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    # criar/registrar tabela Iceberg via writeTo (Spark 3.4+ com Iceberg runtime)\n",
        "    spark.sql('CREATE NAMESPACE IF NOT EXISTS local_iceberg.default')\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "try:\n",
        "    if 'trips_df' in globals():\n",
        "        # writeTo API pode exigir Spark 3.4+ e o runtime iceberg-spark\n",
        "        trips_df.writeTo('local_iceberg.default.iceberg_trips').tableProperty('format-version','2').createOrReplace()\n",
        "        print('Tabela Iceberg criada: local_iceberg.default.iceberg_trips')\n",
        "        print('Contagem Iceberg:', spark.table('local_iceberg.default.iceberg_trips').count())\n",
        "    else:\n",
        "        print('trips_df não existe — não é possível criar a tabela Iceberg')\n",
        "except Exception as e:\n",
        "    print('Falha ao criar tabela Iceberg — verifique se o runtime iceberg-spark está disponível nos jars do Spark')\n",
        "    print('Erro:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Exemplos de operações na tabela Delta\n",
        "Demonstramos `INSERT` (append), `UPDATE` e `DELETE` usando a API Delta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import Row\n",
        "from datetime import datetime\n",
        "\n",
        "if os.path.exists('/tmp/datalake/delta_trips'):\n",
        "    # 1) INSERT (append)\n",
        "    new_rows = [\n",
        "        Row(trip_id='demo-1', pickup_datetime=str(datetime.utcnow()), dropoff_datetime=str(datetime.utcnow()), vendor_id='v_demo', passenger_count=1, trip_distance=1.2, fare_amount=10.0),\n",
        "        Row(trip_id='demo-2', pickup_datetime=str(datetime.utcnow()), dropoff_datetime=str(datetime.utcnow()), vendor_id='v_demo', passenger_count=2, trip_distance=3.4, fare_amount=20.0)\n",
        "    ]\n",
        "    new_df = spark.createDataFrame(new_rows)\n",
        "    new_df.write.format('delta').mode('append').save('/tmp/datalake/delta_trips')\n",
        "    print('Inseridos rows na Delta')\n",
        "    \n",
        "    # 2) UPDATE\n",
        "    try:\n",
        "        from delta.tables import DeltaTable\n",
        "        dt = DeltaTable.forPath(spark, '/tmp/datalake/delta_trips')\n",
        "        dt.update(\"trip_id = 'demo-1'\", {\"fare_amount\": \"fare_amount * 1.5\"})\n",
        "        print('Atualizado demo-1 na Delta')\n",
        "    except Exception as e:\n",
        "        print('Erro no UPDATE Delta (verifique delta-spark estar presente):', e)\n",
        "\n",
        "    # 3) DELETE\n",
        "    try:\n",
        "        dt.delete(\"trip_id = 'demo-2'\")\n",
        "        print('Deletado demo-2 da Delta')\n",
        "    except Exception as e:\n",
        "        print('Erro no DELETE Delta (verifique delta-spark):', e)\n",
        "\n",
        "    print('Contagem final Delta:', spark.read.format('delta').load('/tmp/datalake/delta_trips').count())\n",
        "else:\n",
        "    print('Delta path não existe — pule esta seção')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Exemplos de operações na tabela Iceberg\n",
        "Mostramos `INSERT` (append), `UPDATE` e `DELETE` usando a escrita DataFrame/SQL via catálogo `local_iceberg`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    # INSERT (append) para Iceberg\n",
        "    new_rows_ice = [\n",
        "        {'trip_id':'ice-1', 'pickup_datetime':None, 'dropoff_datetime':None, 'vendor_id':'v_ice', 'passenger_count':1, 'trip_distance':2.2, 'fare_amount':15.0},\n",
        "        {'trip_id':'ice-2', 'pickup_datetime':None, 'dropoff_datetime':None, 'vendor_id':'v_ice', 'passenger_count':3, 'trip_distance':5.1, 'fare_amount':25.0}\n",
        "    ]\n",
        "    new_df_ice = spark.createDataFrame(new_rows_ice)\n",
        "    try:\n",
        "        new_df_ice.writeTo('local_iceberg.default.iceberg_trips').append()\n",
        "        print('Inseridos rows na Iceberg')\n",
        "    except Exception as e:\n",
        "        print('Erro ao inserir via writeTo — pode faltar o runtime iceberg-spark nos jars:', e)\n",
        "\n",
        "    # UPDATE via SQL (se suportado)\n",
        "    try:\n",
        "        spark.sql(\"UPDATE local_iceberg.default.iceberg_trips SET fare_amount = fare_amount * 1.2 WHERE trip_id = 'ice-1'\")\n",
        "        print('Atualizado ice-1 na Iceberg (SQL)')\n",
        "    except Exception as e:\n",
        "        print('UPDATE SQL falhou para Iceberg — verifique suporte no runtime e permissões:', e)\n",
        "\n",
        "    # DELETE via SQL\n",
        "    try:\n",
        "        spark.sql(\"DELETE FROM local_iceberg.default.iceberg_trips WHERE trip_id = 'ice-2'\")\n",
        "        print('Deletado ice-2 da Iceberg (SQL)')\n",
        "    except Exception as e:\n",
        "        print('DELETE SQL falhou para Iceberg:', e)\n",
        "\n",
        "    # Mostrar contagem (se tabela existir)\n",
        "    try:\n",
        "        cnt = spark.table('local_iceberg.default.iceberg_trips').count()\n",
        "        print('Contagem Iceberg:', cnt)\n",
        "    except Exception as e:\n",
        "        print('Não foi possível ler a tabela Iceberg — talvez não esteja criada ou falta runtime:', e)\n",
        "\n",
        "except Exception as e:\n",
        "    print('Seção Iceberg falhou:', e)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Limpeza e leitura final\n",
        "Leitura das tabelas para verificar o estado final."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print('Delta rows sample:')\n",
        "try:\n",
        "    display(spark.read.format('delta').load(delta_path).limit(5).toPandas())\n",
        "except Exception as e:\n",
        "    print('Não foi possível ler Delta para exibir (verifique delta jar):', e)\n",
        "\n",
        "print('\\nIceberg rows sample:')\n",
        "try:\n",
        "    display(spark.table('local_iceberg.default.iceberg_trips').limit(5).toPandas())\n",
        "except Exception as e:\n",
        "    print('Não foi possível ler Iceberg para exibir (verifique iceberg runtime):', e)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
